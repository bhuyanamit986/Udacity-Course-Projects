{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks with PyTorch\n",
    "\n",
    "Deep learning networks tend to be massive with dozens or hundreds of layers, that's where the term \"deep\" comes from. You can build one of these deep networks using only weight matrices as we did in the previous notebook, but in general it's very cumbersome and difficult to implement. PyTorch has a nice module `nn` that provides a nice way to efficiently build large neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import helper\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we're going to build a larger network that can solve a (formerly) difficult problem, identifying text in an image. Here we'll use the MNIST dataset which consists of greyscale handwritten digits. Each image is 28x28 pixels, you can see a sample below\n",
    "\n",
    "<img src='assets/mnist.png'>\n",
    "\n",
    "Our goal is to build a neural network that can take one of these images and predict the digit in the image.\n",
    "\n",
    "First up, we need to get our dataset. This is provided through the `torchvision` package. The code below will download the MNIST dataset, then create training and test datasets for us. Don't worry too much about the details here, you'll learn more about this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "### Run this cell\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the training data loaded into `trainloader` and we make that an iterator with `iter(trainloader)`. Later, we'll use this to loop through the dataset for training, like\n",
    "\n",
    "```python\n",
    "for image, label in trainloader:\n",
    "    ## do things with images and labels\n",
    "```\n",
    "\n",
    "You'll notice I created the `trainloader` with a batch size of 64, and `shuffle=True`. The batch size is the number of images we get in one iteration from the data loader and pass through our network, often called a *batch*. And `shuffle=True` tells it to shuffle the dataset every time we start going through the data loader again. But here I'm just grabbing the first batch so we can check out the data. We can see below that `images` is just a tensor with size `(64, 1, 28, 28)`. So, 64 images per batch, 1 color channel, and 28x28 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what one of the images looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3df5411940>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try to build a simple network for this dataset using weight matrices and matrix multiplications. Then, we'll see how to do it using PyTorch's `nn` module which provides a much more convenient and powerful method for defining network architectures.\n",
    "\n",
    "The networks you've seen so far are called *fully-connected* or *dense* networks. Each unit in one layer is connected to each unit in the next layer. In fully-connected networks, the input to each layer must be a one-dimensional vector (which can be stacked into a 2D tensor as a batch of multiple examples). However, our images are 28x28 2D tensors, so we need to convert them into 1D vectors. Thinking about sizes, we need to convert the batch of images with shape `(64, 1, 28, 28)` to a have a shape of `(64, 784)`, 784 is 28 times 28. This is typically called *flattening*, we flattened the 2D images into 1D vectors.\n",
    "\n",
    "Previously you built a network with one output unit. Here we need 10 output units, one for each digit. We want our network to predict the digit shown in an image, so what we'll do is calculate probabilities that the image is of any one digit or class. This ends up being a discrete probability distribution over the classes (digits) that tells us the most likely class for the image. That means we need 10 output units for the 10 classes (digits). We'll see how to convert the network output into a probability distribution next.\n",
    "\n",
    "> **Exercise:** Flatten the batch of images `images`. Then build a multi-layer network with 784 input units, 256 hidden units, and 10 output units using random tensors for the weights and biases. For now, use a sigmoid activation for the hidden layer. Leave the output layer without an activation, we'll add one that gives us a probability distribution next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution\n",
    "def activation(x):\n",
    "    return 1 / (1+torch.exp(-x))\n",
    "\n",
    "inputs = images.view(images.shape[0], -1)\n",
    "\n",
    "w1 = torch.randn(inputs.shape[1], 256)\n",
    "b1 = torch.randn(256)\n",
    "\n",
    "w2 = torch.randn(256, 10)\n",
    "b2 = torch.randn(10)\n",
    "\n",
    "hidden_inputs = torch.matmul(inputs, w1) + b1\n",
    "hidden_outputs = activation(hidden_inputs)\n",
    "\n",
    "final_inputs = torch.matmul(hidden_outputs, w2) + b2\n",
    "final_outputs = final_inputs\n",
    "\n",
    "out = final_outputs# output of your network, should have shape (64,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10.8365, -11.8443,  -2.1412, -11.8396,   3.3741, -20.7411,\n",
       "          18.2416,  27.7647,  13.6696,   2.7482],\n",
       "        [-11.2000, -13.5560,  -2.6452,  -1.3736,  14.2790, -17.6112,\n",
       "          17.3961,  25.2060,  13.9468, -12.4289],\n",
       "        [ -3.0379, -11.4018,   0.8370,   0.0356,  13.2129, -24.0952,\n",
       "          24.9783,  31.0572,   9.4075,  12.3291],\n",
       "        [-19.3419, -19.5504,  -3.6858,  -5.5194,   7.3672, -23.5262,\n",
       "          29.1827,  24.8864,  15.2119,   1.4151],\n",
       "        [-12.5595, -10.3242,  -3.5387,  -3.7165,  -1.5951, -24.0527,\n",
       "          26.4017,  26.6071,   2.1151,  -2.0474],\n",
       "        [ -5.9816, -13.5340,  -1.1817,  -3.8686,  16.1741, -19.8396,\n",
       "          10.4448,  36.4650,   3.7869,  -2.0411],\n",
       "        [-11.3471,  -8.4298,  -3.4854, -11.8368,  15.4411, -15.9533,\n",
       "          18.2582,  29.5408,   6.7298,  -5.1181],\n",
       "        [ -7.8093,  -6.4989,  -8.1453,  -3.6181,  13.5418, -22.1451,\n",
       "          23.3284,  34.1889,  11.5595,   1.0814],\n",
       "        [-15.6736, -16.0852,   0.5464,  -4.1044,  12.0322, -23.2193,\n",
       "          10.3310,  25.4348,   4.4098,  -7.2103],\n",
       "        [ -7.5235,  -0.0774,   6.5655,  -1.5954,  15.5091, -12.4997,\n",
       "          17.4941,  27.2131,   9.7112,  -4.8306],\n",
       "        [ -6.4593, -15.4445,   2.6050,  -5.6881,   8.6979, -26.5865,\n",
       "          11.1040,  29.8580,  13.3556,  -8.6652],\n",
       "        [-15.6116,  -5.5577,  -4.1229,  -1.4657,   1.1010, -22.3930,\n",
       "          21.6754,  20.8880,   5.0671,  -1.8094],\n",
       "        [-16.6295, -15.5721,  -2.9164,  -5.4730,  21.0997, -20.5798,\n",
       "          16.2718,  35.7123,   4.8753,  -7.2110],\n",
       "        [-20.0172, -15.7461,   2.8268,  -2.6322,   6.0213, -15.3788,\n",
       "          19.8639,  23.4732,   1.4109,  -7.8450],\n",
       "        [-14.4776, -13.0029,   0.2293,  -2.0046,  16.6191, -15.0744,\n",
       "          19.5240,  32.6835,  16.9443,  -7.2346],\n",
       "        [-13.8134,  -6.6123,  -3.8495,  -4.3644,   5.1544, -19.4414,\n",
       "          24.8338,  23.9487,  15.1600,   3.3063],\n",
       "        [-17.8233,  -8.5818,   0.2517,  -1.1590,  -0.7176, -21.0399,\n",
       "          19.0468,  19.6147,   0.3175,  -1.8976],\n",
       "        [-16.6444,  -7.8657,   0.0343,   2.7846,   7.8331, -21.3887,\n",
       "          21.4959,  22.8410,   3.1736,  -2.8793],\n",
       "        [-14.0305, -20.6731,  -5.0207,  -0.7508,   3.2416, -29.0897,\n",
       "          18.6576,  31.8772,   4.5053,  -4.1404],\n",
       "        [-14.4843, -17.8178,   0.2004,  -3.5562,   0.8719, -20.3555,\n",
       "          22.4081,  31.4168,  10.3023, -10.3829],\n",
       "        [-13.6961, -14.6293,   7.8656,   1.2621,  15.9288, -27.0953,\n",
       "           7.8221,  34.5642,   7.4183, -12.3893],\n",
       "        [-11.8138, -13.6442, -10.0698,   0.6007,   7.4892, -15.0591,\n",
       "          16.8712,  24.7537,   4.3663,  -2.5032],\n",
       "        [-18.3773, -12.7623,  -0.2929,   0.4349,  12.2554, -18.5457,\n",
       "          12.6049,  30.4750,  13.2617,  -3.8536],\n",
       "        [-18.8459, -10.5800,   1.8869,  -5.0703,  12.8567, -20.2063,\n",
       "          18.1566,  25.1836,  10.1006,  -9.8849],\n",
       "        [-13.4295, -12.7973,  -3.9066,  -4.2148,  10.8630, -17.0841,\n",
       "          18.6882,  28.4674,  14.1689,  -0.2243],\n",
       "        [-12.6533, -12.1273, -10.4520,  -3.1935,   4.9622, -19.5782,\n",
       "          21.3667,  31.0528,   6.7992,  -4.7061],\n",
       "        [ -7.9790, -10.8919,  -0.3994,  -5.4747,  10.7823, -15.8622,\n",
       "          24.8731,  36.5403,  13.7933,  -0.6881],\n",
       "        [-21.7761,  -3.5649,   0.8686,   2.7189,   1.4772, -21.7478,\n",
       "          20.4604,  22.9782,   3.5176,  -4.0765],\n",
       "        [-10.9065, -13.0472,  -6.9566,  -4.7634,   5.6149, -22.6655,\n",
       "          15.3031,  33.9893,  -1.1481,  -3.0322],\n",
       "        [-11.2930,  -9.3206,  -0.6279,  -3.9618,  13.1510, -18.6220,\n",
       "          21.9089,  26.7997,   7.5340, -12.0620],\n",
       "        [ -5.6236,   2.9659,  -3.4427,   1.0371,   9.0361, -14.9551,\n",
       "          19.9391,  28.6073,   3.5344,  -4.4308],\n",
       "        [-14.6992, -16.5194,  -1.7620,  -2.0801,   6.3799, -19.0602,\n",
       "          25.5581,  32.8329,   4.8117,  -0.0199],\n",
       "        [-13.5495,  -4.4832,  -7.1108,  -7.0233,  16.8181, -15.5433,\n",
       "          22.8312,  18.9753,   6.4799,  -2.4941],\n",
       "        [ -9.8225, -17.4574,  -6.9514,   3.3042,   8.9026, -20.4099,\n",
       "          13.7963,  33.4550,   3.6864,  -2.8116],\n",
       "        [ -2.9281, -16.6622,  -5.6516,   1.7207,  10.6194, -15.7418,\n",
       "          13.1549,  30.9143,  -2.1527,  -9.0129],\n",
       "        [ -9.9914,  -8.4096,  -7.9645,  -2.5762,   5.7216, -24.6412,\n",
       "          16.0727,  37.0811,  -1.0621,  -0.7266],\n",
       "        [-13.6673, -15.0709, -10.6586,  -4.8140,   8.7373, -19.5054,\n",
       "          17.7050,  30.6889,   7.8022,  -2.0190],\n",
       "        [-10.3689, -15.8730,  -2.2641,  -1.2274,   8.0225, -24.3759,\n",
       "          20.1998,  21.6668,  11.0160,   6.2552],\n",
       "        [-10.4023, -17.7305,  -6.5381,  -2.5327,  11.5249, -22.8330,\n",
       "          19.0963,  32.3779,   8.2486,   3.8058],\n",
       "        [ -1.5546, -11.7202,   5.2817,  -7.9066,  21.6353, -22.7277,\n",
       "          12.0116,  34.1468,  11.1424, -12.4088],\n",
       "        [-11.2023,  -3.1935,   1.1624, -11.7994,  12.8740, -14.0379,\n",
       "          13.6614,  26.3624,   1.6271,  -8.1954],\n",
       "        [-20.3840, -13.8870,  -4.2311,  -2.9973,  10.6031, -13.2879,\n",
       "          15.8966,  17.4228,   6.6390,   0.2146],\n",
       "        [-10.5234,  -3.0725,  -1.1201,  -7.6965,  20.2667, -13.4893,\n",
       "           6.1723,  32.1404,  11.4067,  -7.4642],\n",
       "        [-20.9202, -10.9857,  -6.2106,  -3.8496,   4.1195, -15.5790,\n",
       "          17.0711,  29.1622,   7.3683,  -4.0136],\n",
       "        [ -9.1593,  -9.0518,   1.6008,  -0.4867,  11.2848, -11.8922,\n",
       "          18.8666,  25.4331,   6.6463,   1.3369],\n",
       "        [-11.8304, -10.8731,   3.5943,  -1.4958,  12.6862, -21.3041,\n",
       "          22.0637,  27.8103,   7.2653,  -7.9775],\n",
       "        [-13.7526,  -7.2480,  -6.4615,   2.6725,  10.3765, -25.9218,\n",
       "          29.1967,  22.3401,  12.7113,  -5.3714],\n",
       "        [-12.9476, -14.4627,  -7.2270,  -6.6928,   8.2368, -30.9009,\n",
       "          14.4561,  22.5882,  -0.6465, -10.6209],\n",
       "        [-20.1759, -16.4703,   2.9508,   2.6732,  12.3284, -18.0585,\n",
       "          15.6473,  30.1442,   5.2888, -10.9355],\n",
       "        [-11.8643, -11.2218, -14.5552,  -0.4757,  19.1116, -22.4136,\n",
       "           8.9818,  29.7806,   7.0431,   8.5244],\n",
       "        [-15.4450,  -6.7769,   4.4983,  -6.5915,  15.7330, -20.7708,\n",
       "          22.9752,  30.8080,   7.0329,   0.2178],\n",
       "        [-11.5733, -19.3510,  -0.2077,  -6.6505,   8.7565, -20.6749,\n",
       "          18.4232,  22.5335,  11.9335,   1.0279],\n",
       "        [ -9.9430,  -9.5928,  -7.2657,   2.4998,  13.8879, -17.4486,\n",
       "          21.1716,  36.0672,  12.5682,   2.7125],\n",
       "        [-14.2368, -11.8055,   1.1223,  -6.7948,  13.0889, -18.8085,\n",
       "          15.9291,  28.5124,   9.2004, -11.4415],\n",
       "        [-14.6657,  -6.0308,  -6.1590,   0.3721,   5.1748, -26.1402,\n",
       "          23.4345,  22.9468,   3.0277,   2.6271],\n",
       "        [-11.3813,  -9.8963,  -7.3445,  -7.6793,   4.0593, -26.6166,\n",
       "          26.7216,  25.0485,   6.7180,  -1.6356],\n",
       "        [-17.4088, -17.4656,   3.2601,  -3.2204,  10.4646, -25.5044,\n",
       "          24.0093,  31.9956,   8.8017,   7.1005],\n",
       "        [ -7.7065,  -3.8863,  -0.0165,   1.6542,  10.4422, -23.7822,\n",
       "          25.0521,  28.5167,   3.5860,  -0.2212],\n",
       "        [-11.6636,  -8.9988,   2.1258,  -2.2470,   9.0935, -28.8816,\n",
       "          21.8976,  28.8316,  14.5755,   0.8085],\n",
       "        [-13.6161, -20.5912,  -5.2752,  -1.1412,   5.3410, -14.9554,\n",
       "          15.3368,  26.6015,   3.2114,  -1.7628],\n",
       "        [-12.3976, -13.6251,   2.4628,  -5.2746,   6.6959, -17.1355,\n",
       "          18.5039,  19.6765,   9.1475,   2.1845],\n",
       "        [-17.8634, -13.7463,   4.0086,  -4.9096,  18.2136, -17.6568,\n",
       "          19.6119,  30.3954,   4.3696,   2.6736],\n",
       "        [-14.7814, -18.8810,  -9.6531,   0.2449,  15.8683, -21.2604,\n",
       "          16.7238,  30.9170,  11.9394,   4.9634],\n",
       "        [-20.0075,  -9.8024,  -4.8305,  -2.3390,  12.3325, -18.9195,\n",
       "          23.7458,  35.2492,   6.3793,   2.2442]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 10 outputs for our network. We want to pass in an image to our network and get out a probability distribution over the classes that tells us the likely class(es) the image belongs to. Something that looks like this:\n",
    "<img src='assets/image_distribution.png' width=500px>\n",
    "\n",
    "Here we see that the probability for each class is roughly the same. This is representing an untrained network, it hasn't seen any data yet so it just returns a uniform distribution with equal probabilities for each class.\n",
    "\n",
    "To calculate this probability distribution, we often use the [**softmax** function](https://en.wikipedia.org/wiki/Softmax_function). Mathematically this looks like\n",
    "\n",
    "$$\n",
    "\\Large \\sigma(x_i) = \\cfrac{e^{x_i}}{\\sum_k^K{e^{x_k}}}\n",
    "$$\n",
    "\n",
    "What this does is squish each input $x_i$ between 0 and 1 and normalizes the values to give you a proper probability distribution where the probabilites sum up to one.\n",
    "\n",
    "> **Exercise:** Implement a function `softmax` that performs the softmax calculation and returns probability distributions for each example in the batch. Note that you'll need to pay attention to the shapes when doing this. If you have a tensor `a` with shape `(64, 10)` and a tensor `b` with shape `(64,)`, doing `a/b` will give you an error because PyTorch will try to do the division across the columns (called broadcasting) but you'll get a size mismatch. The way to think about this is for each of the 64 examples, you only want to divide by one value, the sum in the denominator. So you need `b` to have a shape of `(64, 1)`. This way PyTorch will divide the 10 values in each row of `a` by the one value in each row of `b`. Pay attention to how you take the sum as well. You'll need to define the `dim` keyword in `torch.sum`. Setting `dim=0` takes the sum across the rows while `dim=1` takes the sum across the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "tensor([ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000])\n",
      "tensor([[ 1.7205e-17,  6.2805e-18,  1.0280e-13,  6.3100e-18,  2.5541e-11,\n",
      "          8.5933e-22,  7.3132e-05,  9.9993e-01,  7.5602e-07,  1.3659e-11],\n",
      "        [ 1.5448e-16,  1.4644e-17,  8.0200e-13,  2.8604e-12,  1.7958e-05,\n",
      "          2.5383e-19,  4.0551e-04,  9.9956e-01,  1.2883e-05,  4.5203e-17],\n",
      "        [ 1.5548e-15,  3.6248e-19,  7.4907e-14,  3.3610e-14,  1.7754e-08,\n",
      "          1.1133e-24,  2.2853e-03,  9.9771e-01,  3.9506e-10,  7.3361e-09],\n",
      "        [ 8.3207e-22,  6.7546e-22,  5.2423e-15,  8.3793e-16,  3.3096e-10,\n",
      "          1.2675e-23,  9.8656e-01,  1.3436e-02,  8.4468e-07,  8.6064e-13],\n",
      "        [ 5.3884e-18,  5.0377e-17,  4.4577e-14,  3.7319e-14,  3.1134e-13,\n",
      "          5.4954e-23,  4.4882e-01,  5.5118e-01,  1.2722e-11,  1.9806e-13],\n",
      "        [ 3.6785e-19,  1.9307e-22,  4.4692e-17,  3.0434e-18,  1.5409e-09,\n",
      "          3.5254e-25,  5.0069e-12,  1.0000e+00,  6.4282e-15,  1.8923e-17],\n",
      "        [ 1.7482e-18,  3.2328e-17,  4.5386e-15,  1.0714e-18,  7.5264e-07,\n",
      "          1.7465e-20,  1.2591e-05,  9.9999e-01,  1.2397e-10,  8.8683e-16],\n",
      "        [ 5.7597e-19,  2.1357e-18,  4.1163e-19,  3.8074e-17,  1.0792e-09,\n",
      "          3.4234e-25,  1.9202e-05,  9.9998e-01,  1.4865e-10,  4.1841e-15],\n",
      "        [ 1.4023e-18,  9.2917e-19,  1.5528e-11,  1.4835e-13,  1.5112e-06,\n",
      "          7.4096e-22,  2.7575e-07,  1.0000e+00,  7.3950e-10,  6.6440e-15],\n",
      "        [ 8.2052e-16,  1.4056e-12,  1.0785e-09,  3.0805e-13,  8.2608e-06,\n",
      "          5.6615e-18,  6.0131e-05,  9.9993e-01,  2.5062e-08,  1.2123e-14],\n",
      "        [ 1.6889e-16,  2.1151e-20,  1.4593e-12,  3.6519e-16,  6.4607e-10,\n",
      "          3.0650e-25,  7.1655e-09,  1.0000e+00,  6.8088e-08,  1.8602e-17],\n",
      "        [ 4.4013e-17,  1.0232e-12,  4.2958e-12,  6.1245e-11,  7.9757e-10,\n",
      "          4.9944e-20,  6.8727e-01,  3.1273e-01,  4.2093e-08,  4.3433e-11],\n",
      "        [ 1.8546e-23,  5.3389e-23,  1.6740e-17,  1.2985e-18,  4.5065e-07,\n",
      "          3.5699e-25,  3.6064e-09,  1.0000e+00,  4.0520e-14,  2.2837e-19],\n",
      "        [ 1.2611e-19,  9.0300e-18,  1.0515e-09,  4.4771e-12,  2.5653e-08,\n",
      "          1.3038e-17,  2.6357e-02,  9.7364e-01,  2.5519e-10,  2.4384e-14],\n",
      "        [ 3.2975e-21,  1.4409e-20,  8.0412e-15,  8.6123e-16,  1.0551e-07,\n",
      "          1.8154e-21,  1.9271e-06,  1.0000e+00,  1.4606e-07,  4.6109e-18],\n",
      "        [ 1.1633e-17,  1.5597e-14,  2.4712e-13,  1.4768e-13,  2.0103e-09,\n",
      "          4.1827e-20,  7.0786e-01,  2.9210e-01,  4.4528e-05,  3.1670e-10],\n",
      "        [ 3.5147e-17,  3.6257e-13,  2.4874e-09,  6.0686e-10,  9.4363e-10,\n",
      "          1.4091e-18,  3.6172e-01,  6.3828e-01,  2.6565e-09,  2.8994e-10],\n",
      "        [ 5.6381e-18,  3.6619e-14,  9.8774e-11,  1.5455e-09,  2.4078e-07,\n",
      "          4.9061e-20,  2.0667e-01,  7.9333e-01,  2.2803e-09,  5.3610e-12],\n",
      "        [ 1.1549e-20,  1.5054e-23,  9.4498e-17,  6.7584e-15,  3.6617e-13,\n",
      "          3.3295e-27,  1.8146e-06,  1.0000e+00,  1.2957e-12,  2.2790e-16],\n",
      "        [ 1.1623e-20,  4.1457e-22,  2.7722e-14,  6.4767e-16,  5.4255e-14,\n",
      "          3.2770e-23,  1.2232e-04,  9.9988e-01,  6.7611e-10,  7.0235e-19],\n",
      "        [ 1.0986e-21,  4.3205e-22,  2.5407e-12,  3.4443e-15,  8.0683e-09,\n",
      "          1.6659e-27,  2.4327e-12,  1.0000e+00,  1.6245e-12,  4.0585e-21],\n",
      "        [ 1.3144e-16,  2.1078e-17,  7.5190e-16,  3.2382e-11,  3.1764e-08,\n",
      "          5.1210e-18,  3.7712e-04,  9.9962e-01,  1.3986e-09,  1.4531e-12],\n",
      "        [ 6.0775e-22,  1.6682e-19,  4.3417e-14,  8.9899e-14,  1.2227e-08,\n",
      "          5.1353e-22,  1.7342e-08,  1.0000e+00,  3.3446e-08,  1.2339e-15],\n",
      "        [ 7.5482e-20,  2.9357e-16,  7.6211e-11,  7.2534e-14,  4.4273e-06,\n",
      "          1.9366e-20,  8.8681e-04,  9.9911e-01,  2.8129e-07,  5.8826e-16],\n",
      "        [ 6.3732e-19,  1.1993e-18,  8.7115e-15,  6.4014e-15,  2.2618e-08,\n",
      "          1.6490e-20,  5.6614e-05,  9.9994e-01,  6.1686e-07,  3.4618e-13],\n",
      "        [ 1.0439e-19,  1.7663e-19,  9.4331e-19,  1.3397e-15,  4.6660e-12,\n",
      "          1.0261e-22,  6.2134e-05,  9.9994e-01,  2.9292e-11,  2.9518e-16],\n",
      "        [ 4.6290e-20,  2.5144e-21,  9.0629e-17,  5.6640e-19,  6.5076e-12,\n",
      "          1.7453e-23,  8.5706e-06,  9.9999e-01,  1.3216e-10,  6.7908e-17],\n",
      "        [ 3.3867e-20,  2.7467e-12,  2.3135e-10,  1.4718e-09,  4.2517e-10,\n",
      "          3.4840e-20,  7.4618e-02,  9.2538e-01,  3.2711e-09,  1.6468e-12],\n",
      "        [ 3.1770e-20,  3.7354e-21,  1.6498e-18,  1.4789e-17,  4.7554e-13,\n",
      "          2.4840e-25,  7.6683e-09,  1.0000e+00,  5.4959e-16,  8.3518e-17],\n",
      "        [ 2.8397e-17,  2.0412e-16,  1.2164e-12,  4.3369e-14,  1.1727e-06,\n",
      "          1.8636e-20,  7.4594e-03,  9.9254e-01,  4.2631e-09,  1.3161e-17],\n",
      "        [ 1.3603e-15,  7.3118e-12,  1.2044e-14,  1.0625e-12,  3.1641e-09,\n",
      "          1.2051e-19,  1.7194e-04,  9.9983e-01,  1.2910e-11,  4.4838e-15],\n",
      "        [ 2.2740e-21,  3.6835e-22,  9.4480e-16,  6.8738e-16,  3.2458e-12,\n",
      "          2.9028e-23,  6.9228e-04,  9.9931e-01,  6.7646e-13,  5.3940e-15],\n",
      "        [ 1.5486e-16,  1.3409e-12,  9.6875e-14,  1.0574e-13,  2.3901e-03,\n",
      "          2.1087e-17,  9.7694e-01,  2.0666e-02,  7.7377e-08,  9.8004e-12],\n",
      "        [ 1.6025e-19,  7.7449e-23,  2.8296e-18,  8.0477e-14,  2.1728e-11,\n",
      "          4.0436e-24,  2.8995e-09,  1.0000e+00,  1.1793e-13,  1.7766e-16],\n",
      "        [ 2.0065e-15,  2.1767e-21,  1.3172e-16,  2.0960e-13,  1.5348e-09,\n",
      "          5.4643e-21,  1.9373e-08,  1.0000e+00,  4.3570e-15,  4.5690e-18],\n",
      "        [ 3.6030e-21,  1.7524e-20,  2.7350e-20,  5.9848e-18,  2.4029e-14,\n",
      "          1.5644e-27,  7.5188e-10,  1.0000e+00,  2.7202e-17,  3.8049e-17],\n",
      "        [ 5.4489e-20,  1.3389e-20,  1.1040e-18,  3.8130e-16,  2.9277e-10,\n",
      "          1.5881e-22,  2.2969e-06,  1.0000e+00,  1.1493e-10,  6.2393e-15],\n",
      "        [ 9.9306e-15,  4.0415e-17,  3.2873e-11,  9.2700e-11,  9.6439e-07,\n",
      "          8.1999e-21,  1.8739e-01,  8.1259e-01,  1.9244e-05,  1.6470e-07],\n",
      "        [ 2.6351e-19,  1.7306e-22,  1.2560e-17,  6.8945e-16,  8.7829e-10,\n",
      "          1.0524e-24,  1.7055e-06,  1.0000e+00,  3.3171e-11,  3.9018e-13],\n",
      "        [ 3.1269e-16,  1.2029e-20,  2.9112e-13,  5.4508e-19,  3.6841e-06,\n",
      "          1.9940e-25,  2.4368e-10,  1.0000e+00,  1.0217e-10,  6.0421e-21],\n",
      "        [ 4.8511e-17,  1.4589e-13,  1.1370e-11,  2.6700e-17,  1.3869e-06,\n",
      "          2.8469e-18,  3.0479e-06,  1.0000e+00,  1.8096e-11,  9.8112e-16],\n",
      "        [ 3.1253e-17,  2.0725e-14,  3.2360e-10,  1.1113e-09,  8.9624e-04,\n",
      "          3.7729e-14,  1.7838e-01,  8.2071e-01,  1.7015e-05,  2.7589e-08],\n",
      "        [ 2.9602e-19,  5.0960e-16,  3.5904e-15,  5.0010e-18,  6.9710e-06,\n",
      "          1.5249e-20,  5.2745e-12,  9.9999e-01,  9.8962e-10,  6.3084e-18],\n",
      "        [ 1.7762e-22,  3.6646e-18,  4.3431e-16,  4.6042e-15,  1.3308e-11,\n",
      "          3.7081e-20,  5.6094e-06,  9.9999e-01,  3.4282e-10,  3.9079e-15],\n",
      "        [ 9.4647e-16,  1.0539e-15,  4.4582e-11,  5.5284e-12,  7.1593e-07,\n",
      "          6.1549e-17,  1.4048e-03,  9.9859e-01,  6.9248e-09,  3.4243e-11],\n",
      "        [ 6.0658e-18,  1.5800e-17,  3.0321e-11,  1.8671e-13,  2.6933e-07,\n",
      "          4.6611e-22,  3.1836e-03,  9.9682e-01,  1.1913e-09,  2.8588e-16],\n",
      "        [ 2.2228e-19,  1.4853e-16,  3.2613e-16,  3.0216e-12,  6.6995e-09,\n",
      "          1.1532e-24,  9.9895e-01,  1.0514e-03,  6.9184e-08,  9.7011e-16],\n",
      "        [ 3.6885e-16,  8.1072e-17,  1.1253e-13,  1.9199e-13,  5.8498e-07,\n",
      "          5.8864e-24,  2.9386e-04,  9.9971e-01,  8.1128e-11,  3.7785e-15],\n",
      "        [ 1.4004e-22,  5.6963e-21,  1.5490e-12,  1.1735e-12,  1.8311e-08,\n",
      "          1.1636e-21,  5.0589e-07,  1.0000e+00,  1.6049e-11,  1.4431e-18],\n",
      "        [ 8.2004e-19,  1.5591e-18,  5.5615e-20,  7.2416e-14,  2.3254e-05,\n",
      "          2.1494e-23,  9.2719e-10,  9.9998e-01,  1.3341e-10,  5.8682e-10],\n",
      "        [ 8.1736e-21,  4.7524e-17,  3.7470e-12,  5.7203e-17,  2.8369e-07,\n",
      "          3.9761e-23,  3.9637e-04,  9.9960e-01,  4.7253e-11,  5.1839e-14],\n",
      "        [ 1.5155e-15,  6.3495e-19,  1.3078e-10,  2.0821e-13,  1.0225e-06,\n",
      "          1.6895e-19,  1.6138e-02,  9.8384e-01,  2.4515e-05,  4.4998e-10],\n",
      "        [ 1.0425e-20,  1.4796e-20,  1.5163e-19,  2.6416e-15,  2.3318e-10,\n",
      "          5.7335e-24,  3.3957e-07,  1.0000e+00,  6.2309e-11,  3.2680e-15],\n",
      "        [ 2.7181e-19,  3.0916e-18,  1.2724e-12,  4.6375e-16,  2.0029e-07,\n",
      "          2.8105e-21,  3.4289e-06,  1.0000e+00,  4.1011e-09,  4.4488e-18],\n",
      "        [ 1.7594e-17,  9.8963e-14,  8.7053e-14,  5.9731e-11,  7.2779e-09,\n",
      "          1.8283e-22,  6.1958e-01,  3.8042e-01,  8.5020e-10,  5.6954e-10],\n",
      "        [ 2.3847e-17,  1.0528e-16,  1.3508e-15,  9.6648e-16,  1.2112e-10,\n",
      "          5.7650e-24,  8.4199e-01,  1.5801e-01,  1.7292e-09,  4.0732e-13],\n",
      "        [ 3.4978e-22,  3.3047e-22,  3.3128e-13,  5.0784e-16,  4.4570e-10,\n",
      "          1.0664e-25,  3.3997e-04,  9.9966e-01,  8.4504e-11,  1.5418e-11],\n",
      "        [ 1.7991e-16,  8.2069e-15,  3.9336e-13,  2.0912e-12,  1.3708e-08,\n",
      "          1.8772e-23,  3.0335e-02,  9.6966e-01,  1.4433e-11,  3.2055e-13],\n",
      "        [ 2.5867e-18,  3.7158e-17,  2.5201e-12,  3.1792e-14,  2.6757e-09,\n",
      "          8.6116e-26,  9.7321e-04,  9.9903e-01,  6.4306e-07,  6.7505e-13],\n",
      "        [ 3.4176e-18,  3.1950e-21,  1.4326e-14,  8.9431e-13,  5.8437e-10,\n",
      "          8.9554e-19,  1.2817e-05,  9.9999e-01,  6.9473e-11,  4.8033e-13],\n",
      "        [ 8.9790e-15,  2.6313e-15,  2.5530e-08,  1.1136e-11,  1.7598e-06,\n",
      "          7.8636e-17,  2.3637e-01,  7.6361e-01,  2.0425e-05,  1.9327e-08],\n",
      "        [ 1.1002e-21,  6.7533e-20,  3.4703e-12,  4.6480e-16,  5.1229e-06,\n",
      "          1.3528e-21,  2.0740e-05,  9.9997e-01,  4.9792e-12,  9.1327e-13],\n",
      "        [ 1.4237e-20,  2.3604e-22,  2.4024e-18,  4.7781e-14,  2.9135e-07,\n",
      "          2.1859e-23,  6.8543e-07,  1.0000e+00,  5.7299e-09,  5.3519e-12],\n",
      "        [ 1.0054e-24,  2.7185e-20,  3.9229e-18,  4.7385e-17,  1.1153e-10,\n",
      "          2.9842e-24,  1.0095e-05,  9.9999e-01,  2.8971e-13,  4.6358e-15]])\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    ## TODO: Implement the softmax function here\n",
    "    return torch.exp(x) / torch.sum(torch.exp(x), dim = 1).view(-1, 1)\n",
    "\n",
    "# Here, out should be the output of the network in the previous excercise with shape (64,10)\n",
    "probabilities = softmax(out)\n",
    "\n",
    "# Does it have the right shape? Should be (64, 10)\n",
    "print(probabilities.shape)\n",
    "# Does it sum to 1?\n",
    "print(probabilities.sum(dim=1))\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building networks with PyTorch\n",
    "\n",
    "PyTorch provides a module `nn` that makes building networks much simpler. Here I'll show you how to build the same one as above with 784 inputs, 256 hidden units, 10 output units and a softmax output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "        # Define sigmoid activation and softmax output \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through this bit by bit.\n",
    "\n",
    "```python\n",
    "class Network(nn.Module):\n",
    "```\n",
    "\n",
    "Here we're inheriting from `nn.Module`. Combined with `super().__init__()` this creates a class that tracks the architecture and provides a lot of useful methods and attributes. It is mandatory to inherit from `nn.Module` when you're creating a class for your network. The name of the class itself can be anything.\n",
    "\n",
    "```python\n",
    "self.hidden = nn.Linear(784, 256)\n",
    "```\n",
    "\n",
    "This line creates a module for a linear transformation, $x\\mathbf{W} + b$, with 784 inputs and 256 outputs and assigns it to `self.hidden`. The module automatically creates the weight and bias tensors which we'll use in the `forward` method. You can access the weight and bias tensors once the network (`net`) is created with `net.hidden.weight` and `net.hidden.bias`.\n",
    "\n",
    "```python\n",
    "self.output = nn.Linear(256, 10)\n",
    "```\n",
    "\n",
    "Similarly, this creates another linear transformation with 256 inputs and 10 outputs.\n",
    "\n",
    "```python\n",
    "self.sigmoid = nn.Sigmoid()\n",
    "self.softmax = nn.Softmax(dim=1)\n",
    "```\n",
    "\n",
    "Here I defined operations for the sigmoid activation and softmax output. Setting `dim=1` in `nn.Softmax(dim=1)` calculates softmax across the columns.\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "```\n",
    "\n",
    "PyTorch networks created with `nn.Module` must have a `forward` method defined. It takes in a tensor `x` and passes it through the operations you defined in the `__init__` method.\n",
    "\n",
    "```python\n",
    "x = self.hidden(x)\n",
    "x = self.sigmoid(x)\n",
    "x = self.output(x)\n",
    "x = self.softmax(x)\n",
    "```\n",
    "\n",
    "Here the input tensor `x` is passed through each operation a reassigned to `x`. We can see that the input tensor goes through the hidden layer, then a sigmoid function, then the output layer, and finally the softmax function. It doesn't matter what you name the variables here, as long as the inputs and outputs of the operations match the network architecture you want to build. The order in which you define things in the `__init__` method doesn't matter, but you'll need to sequence the operations correctly in the `forward` method.\n",
    "\n",
    "Now we can create a `Network` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the network and look at it's text representation\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can define the network somewhat more concisely and clearly using the `torch.nn.functional` module. This is the most common way you'll see networks defined as many operations are simple element-wise functions. We normally import this module as `F`, `import torch.nn.functional as F`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Hidden layer with sigmoid activation\n",
    "        x = F.sigmoid(self.hidden(x))\n",
    "        # Output layer with softmax activation\n",
    "        x = F.softmax(self.output(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "So far we've only been looking at the softmax activation, but in general any function can be used as an activation function. The only requirement is that for a network to approximate a non-linear function, the activation functions must be non-linear. Here are a few more examples of common activation functions: Tanh (hyperbolic tangent), and ReLU (rectified linear unit).\n",
    "\n",
    "<img src=\"assets/activation.png\" width=700px>\n",
    "\n",
    "In practice, the ReLU function is used almost exclusively as the activation function for hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn to Build a Network\n",
    "\n",
    "<img src=\"assets/mlp_mnist.png\" width=600px>\n",
    "\n",
    "> **Exercise:** Create a network with 784 input units, a hidden layer with 128 units and a ReLU activation, then a hidden layer with 64 units and a ReLU activation, and finally an output layer with a softmax activation as shown above. You can use a ReLU activation with the `nn.ReLU` module or `F.relu` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Your solution here\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DeepNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_1 = nn.Linear(784, 128)\n",
    "        self.hidden_2 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden_1(x))\n",
    "        x = F.relu(self.hidden_2(x))\n",
    "        x = F.softmax(self.output(x), dim = 1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing weights and biases\n",
    "\n",
    "The weights and such are automatically initialized for you, but it's possible to customize how they are initialized. The weights and biases are tensors attached to the layer you defined, you can get them with `model.fc1.weight` for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.2301e-02, -2.2688e-02, -3.1285e-02,  ..., -2.1978e-02,\n",
      "          2.4275e-04, -5.6450e-03],\n",
      "        [-3.0336e-02,  3.3079e-03, -2.3281e-02,  ...,  1.2813e-02,\n",
      "          2.1307e-02, -5.1737e-04],\n",
      "        [ 2.9205e-02,  8.0421e-03,  1.6671e-02,  ..., -6.3676e-03,\n",
      "         -5.4038e-03,  3.6860e-03],\n",
      "        ...,\n",
      "        [-2.2024e-03,  2.8358e-02, -3.5306e-02,  ..., -2.6086e-02,\n",
      "         -2.2109e-02,  6.0669e-03],\n",
      "        [-7.4859e-04, -2.5435e-02, -9.1953e-03,  ...,  3.5332e-03,\n",
      "         -1.1952e-02, -7.7248e-03],\n",
      "        [ 2.0787e-02,  1.2064e-02, -2.7679e-02,  ..., -2.2447e-02,\n",
      "         -3.9733e-03,  2.8183e-02]])\n",
      "Parameter containing:\n",
      "tensor(1.00000e-02 *\n",
      "       [ 0.2126,  2.2779, -3.5430, -1.8864, -0.8681, -1.8371, -0.1951,\n",
      "         0.4597, -0.1521, -2.8645, -2.1793,  1.1146,  0.6239, -1.3354,\n",
      "         1.5463,  0.4449,  2.2870, -0.7872, -2.2986, -0.5049,  3.2804,\n",
      "        -3.1871, -2.9465,  0.7562, -3.0290,  3.4886,  3.4886,  0.7238,\n",
      "        -0.4725, -0.5373, -2.0806, -0.0816, -1.4233,  0.5616,  2.2382,\n",
      "         0.9823,  2.0648,  1.4592, -0.6516,  1.5273,  2.3629, -3.5319,\n",
      "        -0.0439, -3.0910,  2.3800,  0.2680,  2.4079, -2.3980,  1.9356,\n",
      "        -1.8024, -0.6208,  2.6752,  2.3871, -2.7764, -2.3462,  2.6026,\n",
      "         0.1262, -2.9106, -3.2760, -1.3211, -3.4886, -2.1377,  0.2111,\n",
      "        -0.5275, -3.5657,  1.1399,  0.4201, -2.5175,  1.9148, -1.5347,\n",
      "        -0.8319, -0.9106,  1.1492,  0.1210,  1.2561,  2.6164,  0.1614,\n",
      "        -3.0663,  1.1599,  0.7702,  1.3120,  0.4066, -2.7962,  2.6907,\n",
      "         2.2059,  2.2662, -2.3908, -1.6366, -3.0707, -1.0541,  3.3516,\n",
      "         2.7200,  0.3524, -2.9514, -1.1642, -1.7716,  0.3762,  1.9043,\n",
      "        -3.5550,  1.6877, -2.7088, -0.2605,  1.5429,  1.7719,  1.2987,\n",
      "        -2.2787,  3.5538, -0.2174, -1.0168, -1.4971,  0.1158, -3.3046,\n",
      "         2.5135,  2.5326,  0.4179,  0.2305,  0.7514, -1.3521, -2.9655,\n",
      "         3.3589,  2.4137, -0.3977, -3.1610,  3.5323, -2.0392, -2.9304,\n",
      "         2.6838,  1.0505])\n"
     ]
    }
   ],
   "source": [
    "model = DeepNet()\n",
    "print(model.hidden_1.weight)\n",
    "print(model.hidden_1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For custom initialization, we want to modify these tensors in place. These are actually autograd *Variables*, so we need to get back the actual tensors with `model.fc1.weight.data`. Once we have the tensors, we can fill them with zeros (for biases) or random normal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set biases to all zeros\n",
    "model.hidden_1.bias.data.fill_(0)\n",
    "model.hidden_2.bias.data.fill_(0)\n",
    "model.output.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.00000e-02 *\n",
       "       [[ 1.1666,  1.1940,  0.2718,  0.5742, -0.1857,  0.7277, -0.2446,\n",
       "          0.2650,  0.6973,  0.3887, -1.4827, -0.8345, -0.8439,  0.6894,\n",
       "         -0.3356,  2.5944,  0.1235, -1.1824, -1.0743,  1.0022, -0.2656,\n",
       "         -0.4675,  1.9668,  1.4376,  2.1017,  1.1097,  0.8545,  0.5390,\n",
       "          1.3134, -1.2846,  0.0767, -0.2165,  1.9408, -0.0724, -1.8549,\n",
       "          0.9178, -1.8939,  1.2293, -0.7933,  0.0171,  0.5119, -1.9102,\n",
       "          0.8859, -0.0844,  0.2996, -1.2436, -0.8876, -0.3684,  0.2900,\n",
       "          1.9677, -0.2453, -0.7381,  0.0843, -0.3105,  0.8456, -1.6115,\n",
       "          0.0111, -1.9568,  0.9493, -0.5936,  0.7809,  0.3389,  0.2160,\n",
       "         -0.4916],\n",
       "        [-0.4930,  1.7857,  0.7210,  0.2521,  0.7490, -0.3108,  1.3058,\n",
       "          0.6461,  0.7516,  0.7687,  0.9099,  1.4688,  2.2363,  0.7579,\n",
       "         -0.3397,  1.0794,  0.2266, -1.3102, -0.1766,  0.2430,  2.1953,\n",
       "         -1.6308,  1.6523,  0.9798, -0.5246,  0.0270,  0.7294,  2.0915,\n",
       "          0.3237,  1.1348,  0.5996, -2.0032, -0.6693,  0.9799, -0.1325,\n",
       "         -0.8669,  0.9709,  0.0198, -0.4425,  0.1865, -1.2181, -1.5084,\n",
       "         -0.2704,  0.6924,  0.4190, -0.6467, -0.3299, -0.6061,  0.8641,\n",
       "         -1.5860,  1.8023, -0.2075, -0.2631,  0.2301,  1.3748, -0.7321,\n",
       "          0.3619, -0.3652, -0.8520,  0.1265,  0.6517,  0.6065, -0.4979,\n",
       "          1.2472],\n",
       "        [-0.2041, -0.4564, -0.9795,  1.1658, -0.3126, -0.7516,  0.0816,\n",
       "         -1.3026, -0.5900, -2.2803, -0.1265,  1.6474,  0.0703,  0.0241,\n",
       "          0.9226, -1.0714, -0.2406, -0.5073,  0.2421,  0.1921,  0.7148,\n",
       "         -0.1391, -1.3041,  1.8767, -0.4337,  0.7118, -1.5001, -0.4084,\n",
       "          0.7395,  1.2282, -0.3041,  0.7370, -0.8906,  1.0397, -1.1177,\n",
       "         -0.8434, -1.6258,  0.2572,  0.3646, -0.0707, -1.0608,  0.4000,\n",
       "         -0.9367, -0.6777,  0.8788,  1.4163,  2.0110, -2.0207, -0.2604,\n",
       "         -0.1526, -0.8547, -0.8312,  0.8939,  1.0918,  0.4950,  0.1112,\n",
       "         -0.7467,  0.0367,  1.2285, -0.3597,  0.8751, -0.5134, -0.4250,\n",
       "          2.3584],\n",
       "        [-0.0597,  1.2494, -0.5296,  0.6418, -0.6388, -1.3405,  0.1642,\n",
       "          0.3910,  0.5528,  1.1187,  1.8606, -1.1436,  0.7703,  0.5922,\n",
       "         -0.0755,  0.6869, -1.4242,  0.8749,  0.4782,  1.9772, -0.4384,\n",
       "          0.2291, -2.0344, -0.0395,  0.8813,  0.0181,  0.2158,  0.9922,\n",
       "         -0.8844, -0.8701,  1.1181,  0.4917, -0.8746, -1.4318,  1.6803,\n",
       "          1.9416, -0.0134, -0.5623,  0.4200,  0.4982, -0.5973, -2.0118,\n",
       "         -2.1819, -0.7002,  1.5243, -0.8064, -1.0171, -0.0728,  0.7526,\n",
       "          0.5345, -0.8361, -0.3992, -0.0404, -0.3047,  1.1073,  0.2968,\n",
       "          1.4353,  0.0495,  0.1315, -0.1388, -0.3584, -0.7218, -1.7859,\n",
       "         -0.0032],\n",
       "        [ 1.5094, -0.0634,  1.9072,  0.5995, -2.0254, -1.1200,  0.0491,\n",
       "          1.6140,  1.4025, -0.3175,  1.9337, -0.6874,  0.4996, -1.2466,\n",
       "         -0.3607, -1.4220, -0.4376, -0.6095,  0.1945, -0.3160, -0.7572,\n",
       "          1.7157, -0.2307,  0.1599, -0.0819,  1.4341,  0.4445,  0.8179,\n",
       "         -1.3325,  0.6620,  0.1071,  0.1067, -1.5719,  0.2004,  0.4020,\n",
       "         -0.7610,  0.7957,  1.3792, -0.1934, -1.1979, -0.5493, -1.2271,\n",
       "         -0.6430, -0.8812,  0.3047, -0.1434,  0.2167,  0.7034, -0.8721,\n",
       "          0.0891,  0.2031, -0.8657, -0.7540, -1.4080,  0.2647, -1.0070,\n",
       "         -0.9758, -0.0340,  0.6615,  0.4255,  0.0103,  0.6618,  1.0629,\n",
       "         -2.7882],\n",
       "        [ 0.2006,  0.1008,  0.7155, -0.5049, -1.4976, -0.5720,  0.5862,\n",
       "         -0.5627, -0.9058, -0.6508, -0.4610, -0.2899,  0.5056, -0.0320,\n",
       "         -0.1492, -0.2893,  0.5520,  0.7917, -0.5516, -0.4221,  0.5288,\n",
       "          1.2915,  2.2082,  0.4232,  0.3461,  0.8253,  0.0631, -0.7716,\n",
       "         -0.4555, -1.1059, -0.7009,  0.7343,  0.0543, -1.5488, -1.4366,\n",
       "         -1.8139, -0.4311, -0.7529,  0.5499, -0.9644, -0.5057,  0.3836,\n",
       "          0.2042, -0.1506, -0.0810,  1.0676,  0.3734, -0.6016,  0.9543,\n",
       "         -1.2078,  0.2712, -0.8280,  1.2283, -0.0676,  2.2994,  0.6459,\n",
       "         -1.4313,  0.2149,  0.1144,  0.6954,  0.4071,  0.2997,  0.3424,\n",
       "         -1.2326],\n",
       "        [-1.4228,  0.2239,  1.3576, -0.3606,  0.1952, -0.0309, -2.1492,\n",
       "         -0.1437,  0.2275,  0.7051, -0.8033, -1.1142, -0.5024, -0.3043,\n",
       "         -0.3937,  1.5907,  0.7546,  0.4070, -0.9339,  0.0660, -0.5743,\n",
       "          2.5009,  2.0092, -0.1756,  0.0325, -2.0641,  1.2151, -1.0471,\n",
       "          0.2828,  0.0171,  1.1028, -0.0282, -1.2556, -0.8235,  0.2235,\n",
       "          0.6192, -0.1250, -1.7448, -0.9815, -0.4995, -1.6134,  0.4600,\n",
       "         -0.9433,  0.0564, -0.4056, -0.3104, -1.8542, -1.2892, -2.5035,\n",
       "         -0.4164,  1.4876, -0.0801,  0.2349, -0.2278,  1.4736,  0.9150,\n",
       "         -1.1023,  0.3562,  0.2195, -0.0339,  0.5170,  0.7890, -0.3789,\n",
       "          1.3861],\n",
       "        [-1.4135,  1.6896,  2.4925,  0.8080,  0.4479, -0.1169,  0.0904,\n",
       "          0.1052, -1.2607, -1.1253, -0.6720, -0.9458,  0.2817,  0.0918,\n",
       "          0.6939,  0.3399, -1.5917, -1.5479, -0.2616, -0.2689,  0.1738,\n",
       "          0.4739, -0.0832,  0.1535,  1.1530, -1.4198,  0.6118, -0.2955,\n",
       "         -1.7384, -0.3763,  1.6314,  1.6483,  0.0122, -0.5404,  0.7506,\n",
       "         -1.0581, -1.5388, -1.8018, -0.3502, -0.0393,  1.5468, -1.6391,\n",
       "         -0.8217,  0.8279,  0.8662,  0.8371, -0.3523,  0.9010,  0.3953,\n",
       "          1.1513,  0.4469, -0.4290,  1.9601,  0.1747,  0.3233, -0.1352,\n",
       "         -1.0922,  1.1045,  1.7750, -1.4578, -0.4892,  0.1217,  0.1065,\n",
       "          0.1124],\n",
       "        [ 0.4188,  0.1137,  1.9612, -0.3719, -0.2129,  0.2498, -0.9422,\n",
       "         -0.3113,  1.2619,  1.6784, -1.0607, -0.4620,  1.2973,  0.8409,\n",
       "         -0.1719,  1.2223,  0.5053, -0.5517, -0.0767, -0.2469,  0.7996,\n",
       "          0.3624,  1.4308, -1.0695, -1.1270, -2.0144, -0.4607, -1.9427,\n",
       "          0.4446, -1.5938,  0.8625, -1.1213, -1.1615, -0.5241, -1.9594,\n",
       "         -1.0755, -0.9168, -1.7263, -0.4063,  0.1971,  1.1730, -0.8220,\n",
       "          0.2192,  1.3125,  0.9569,  0.2700, -0.1142, -0.2912, -0.4961,\n",
       "         -0.6860,  0.8415, -0.2921,  0.0335,  0.9546,  0.5222, -0.4572,\n",
       "         -0.2184,  0.7742,  1.0637,  0.7125, -0.4322,  0.4307,  0.3875,\n",
       "         -1.3584],\n",
       "        [ 0.5266,  0.7017, -0.6161,  1.7879, -0.7184, -0.6706,  0.3830,\n",
       "          0.9845, -0.8241, -0.6206, -1.1171, -0.5256, -0.7839, -0.2722,\n",
       "         -0.1255,  0.4335,  1.1461,  1.0399,  0.0480, -0.4287, -0.8390,\n",
       "          0.6765,  1.2395,  0.9488,  0.9423, -0.7591, -1.1146, -0.0707,\n",
       "         -1.5946,  0.8180,  1.1730,  0.4206, -0.3885,  0.2935, -0.1228,\n",
       "          0.3005,  1.2433, -1.0487,  0.1001,  2.8207, -0.2938, -0.7127,\n",
       "         -1.5834,  1.7266,  0.7255, -1.2053,  0.5202, -0.6938,  1.3985,\n",
       "          1.5854,  1.1344, -0.3134, -1.1920,  0.8821,  0.0100, -0.6057,\n",
       "          0.2846,  0.5991, -0.8504,  1.6080,  0.3408, -1.9121,  1.2537,\n",
       "         -0.5698]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample from random normal with standard dev = 0.01\n",
    "model.hidden_1.weight.data.normal_(std=0.01)\n",
    "model.hidden_2.weight.data.normal_(std=0.01)\n",
    "model.output.weight.data.normal_(std=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "Now that we have a network, let's see what happens when we pass in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3d9b809be0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 224,
       "width": 423
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Grab some data \n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Resize images into a 1D vector, new shape is (batch size, color channels, image pixels) \n",
    "images.resize_(64, 1, 784)\n",
    "# or images.resize_(images.shape[0], 1, 784) to automatically get batch size\n",
    "\n",
    "# Forward pass through the network\n",
    "img_idx = 0\n",
    "ps = model.forward(images[img_idx,:])\n",
    "\n",
    "img = images[img_idx]\n",
    "# print(img)\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, our network has basically no idea what this digit is. It's because we haven't trained it yet, all the weights are random!\n",
    "\n",
    "### Using `nn.Sequential`\n",
    "\n",
    "PyTorch provides a convenient way to build networks like this where a tensor is passed sequentially through operations, `nn.Sequential` ([documentation](https://pytorch.org/docs/master/nn.html#torch.nn.Sequential)). Using this to build the equivalent network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): Softmax()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3d981d1518>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 224,
       "width": 423
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "print(model)\n",
    "\n",
    "# Forward pass through the network and display output\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "ps = model.forward(images[0,:])\n",
    "helper.view_classify(images[0].view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here our model is the same as before: 784 input units, a hidden layer with 128 units, ReLU activation, 64 unit hidden layer, another ReLU, then the output layer with 10 units, and the softmax output.\n",
    "\n",
    "The operations are availble by passing in the appropriate index. For example, if you want to get first Linear operation and look at the weights, you'd use `model[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=784, out_features=128, bias=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 3.4795e-02, -1.7978e-02,  1.6037e-02,  ...,  2.3333e-03,\n",
       "         -1.6298e-03,  1.2461e-02],\n",
       "        [ 1.1621e-02, -2.7603e-02,  1.2730e-02,  ...,  3.4627e-02,\n",
       "          2.6592e-02,  8.6925e-03],\n",
       "        [ 1.7169e-02,  1.0230e-02, -5.6280e-03,  ..., -1.0018e-02,\n",
       "          1.1030e-02, -2.0451e-02],\n",
       "        ...,\n",
       "        [-2.6273e-02, -2.1817e-02, -1.7884e-02,  ..., -2.4257e-02,\n",
       "          5.6083e-03,  2.8041e-02],\n",
       "        [ 3.1956e-02,  6.4105e-03,  1.5225e-02,  ..., -1.6747e-02,\n",
       "          1.6040e-02, -1.0444e-02],\n",
       "        [-6.9217e-03,  2.4586e-02, -2.3833e-02,  ...,  1.2395e-02,\n",
       "          1.9668e-02,  2.0165e-03]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model[0])\n",
    "model[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass in an `OrderedDict` to name the individual layers and operations, instead of using incremental integers. Note that dictionary keys must be unique, so _each operation must have a different name_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (softmax): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "model = nn.Sequential(OrderedDict([\n",
    "                      ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "                      ('relu1', nn.ReLU()),\n",
    "                      ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "                      ('relu2', nn.ReLU()),\n",
    "                      ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
    "                      ('softmax', nn.Softmax(dim=1))]))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can access layers either by integer or the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=784, out_features=128, bias=True)\n",
      "Linear(in_features=784, out_features=128, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(model[0])\n",
    "print(model.fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook, we'll see how we can train a neural network to accuractly predict the numbers appearing in the MNIST images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
